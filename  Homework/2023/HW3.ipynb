{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6313c500e458466896364a0fcdb7fd72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01c98be9b3d94d50ad9c1506ad0fa130","IPY_MODEL_167e711dbccf429ca8c519ef4e6c496a","IPY_MODEL_abade8255bb44b2aa2104b9c4861879d"],"layout":"IPY_MODEL_edf73a3d39af4c9c9f2f7692312f87e7"}},"01c98be9b3d94d50ad9c1506ad0fa130":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6723bef5bf1454db80f378ab36692ac","placeholder":"​","style":"IPY_MODEL_21f1e008c2534388828f97f42f24453b","value":"100%"}},"167e711dbccf429ca8c519ef4e6c496a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6c01a3c087b49ff878e410157579b60","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6ef97e9262e4a26975f8521e343ad6d","value":170498071}},"abade8255bb44b2aa2104b9c4861879d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_491cac5368fc4a1f8c0aeff90891538a","placeholder":"​","style":"IPY_MODEL_f9343a7a909340e0887ff02d78c037b4","value":" 170498071/170498071 [00:03&lt;00:00, 52817802.31it/s]"}},"edf73a3d39af4c9c9f2f7692312f87e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6723bef5bf1454db80f378ab36692ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21f1e008c2534388828f97f42f24453b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6c01a3c087b49ff878e410157579b60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ef97e9262e4a26975f8521e343ad6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"491cac5368fc4a1f8c0aeff90891538a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9343a7a909340e0887ff02d78c037b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["#**Deep Learning Homework 3: Convolutional Neural Networks**\n","### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n","### 2nd semester - 6 ECTS\n","### Prof. Alessandro Sperduti, Prof. Nicolò Navarin and Dr. Luca Pasa\n","---\n","In this homework, we will explore how to develop a simple Convolutional Neural Network for image classification. We will use the CIFAR-10 dataset. In the first part, we will learn how to develop a simple CNN, while in the second part we will explore the impact of various hyper-parameters on the learning performances."],"metadata":{"id":"uqSn7VUKVmwX"}},{"cell_type":"markdown","source":["# Requirements\n","Let's start importing the libraries we will need and setting a couple of environmental variables."],"metadata":{"id":"5FEro6dyR9ap"}},{"cell_type":"markdown","source":["**Disclaimer**: the notebook has been tested in Colab with Python `3.9.16`. Some parts *may* not work as expected using different packages and Python versions."],"metadata":{"id":"JQpsgj-C0E1N"}},{"cell_type":"code","source":["print(\"You are using:\")\n","!python --version\n"],"metadata":{"id":"RfA1bRJH0kZN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681829349892,"user_tz":-330,"elapsed":116,"user":{"displayName":"Nikhil K C","userId":"09904819297436616315"}},"outputId":"834cac94-305b-4252-d233-759f62750366"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["You are using:\n","Python 3.9.16\n"]}]},{"cell_type":"code","source":["# They've just released torch 2.0, but we will use the more stable torch=1.13 along with other supporting libriaries\n","!pip3 install pandas~=1.5 torch~=1.13 torchinfo torchdata~=0.5 torchtext~=0.14 torchvision~=0.14 torchaudio~=0.13"],"metadata":{"id":"3jdp8UnEVoL0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681829487153,"user_tz":-330,"elapsed":102920,"user":{"displayName":"Nikhil K C","userId":"09904819297436616315"}},"outputId":"75f57747-81f0-4b23-994f-0ca7c8de6a1f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas~=1.5 in /usr/local/lib/python3.9/dist-packages (1.5.3)\n","Collecting torch~=1.13\n","  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchinfo\n","  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n","Requirement already satisfied: torchdata~=0.5 in /usr/local/lib/python3.9/dist-packages (0.6.0)\n","Requirement already satisfied: torchtext~=0.14 in /usr/local/lib/python3.9/dist-packages (0.15.1)\n","Requirement already satisfied: torchvision~=0.14 in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n","Collecting torchaudio~=0.13\n","  Downloading torchaudio-0.13.1-cp39-cp39-manylinux1_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.5) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.5) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.5) (2.8.2)\n","Collecting nvidia-cublas-cu11==11.10.3.66\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch~=1.13) (4.5.0)\n","Collecting nvidia-cudnn-cu11==8.5.0.96\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch~=1.13) (67.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch~=1.13) (0.40.0)\n","Collecting torchdata~=0.5\n","  Downloading torchdata-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from torchdata~=0.5) (1.26.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchdata~=0.5) (2.27.1)\n","Collecting portalocker>=2.0.0\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Collecting torchtext~=0.14\n","  Downloading torchtext-0.14.1-cp39-cp39-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext~=0.14) (4.65.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision~=0.14) (8.4.0)\n","Collecting torchvision~=0.14\n","  Downloading torchvision-0.15.1-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torchvision-0.14.1-cp39-cp39-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas~=1.5) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata~=0.5) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata~=0.5) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata~=0.5) (2.0.12)\n","Installing collected packages: torchinfo, portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision, torchtext, torchdata, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0+cu118\n","    Uninstalling torch-2.0.0+cu118:\n","      Successfully uninstalled torch-2.0.0+cu118\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.1+cu118\n","    Uninstalling torchvision-0.15.1+cu118:\n","      Successfully uninstalled torchvision-0.15.1+cu118\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.1\n","    Uninstalling torchtext-0.15.1:\n","      Successfully uninstalled torchtext-0.15.1\n","  Attempting uninstall: torchdata\n","    Found existing installation: torchdata 0.6.0\n","    Uninstalling torchdata-0.6.0:\n","      Successfully uninstalled torchdata-0.6.0\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.0.1+cu118\n","    Uninstalling torchaudio-2.0.1+cu118:\n","      Successfully uninstalled torchaudio-2.0.1+cu118\n","Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 torch-1.13.1 torchaudio-0.13.1 torchdata-0.5.1 torchinfo-1.7.2 torchtext-0.14.1 torchvision-0.14.1\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import Conv2d, MaxPool2d, Linear\n","from torchinfo import summary\n","import torch.nn.functional as F\n","from torchvision.datasets import CIFAR10\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.transforms import Lambda\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from timeit import default_timer as timer\n","import warnings\n","warnings.filterwarnings(\"ignore\") #Conflict of nn.functional.act_f and torch.act_f\n","\n","print(f\"{torch.__version__=}\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"{device=}\")\n","!nvidia-smi --format=csv --query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\n","\n","# Set seed for reproducibility\n","torch.manual_seed(43)\n","rng = np.random.default_rng(seed=4242)"],"metadata":{"id":"ltsxWUQTVt8Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681829489008,"user_tz":-330,"elapsed":1954,"user":{"displayName":"Nikhil K C","userId":"09904819297436616315"}},"outputId":"54717450-1bf1-417f-913f-f074f83cd78c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.__version__='1.13.1+cu117'\n","device=device(type='cuda')\n","index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n","0, Tesla T4, 525.85.12, 15360 MiB, 3 MiB, 15098 MiB\n"]}]},{"cell_type":"markdown","source":["# Exercise 3.1: Simple CNN"],"metadata":{"id":"PWawaA5OyrEe"}},{"cell_type":"markdown","source":["## Data Loading and Preprocessing [TO COMPLETE]\n","\n","\n","We will use the `CIFAR-10` dataset.The dataset consists of $60.000$ images in $10$ classes, with $6.000$ images per class. There are $50.000$ training images and $10.000$ test images. Each sample is a $32\\times32$ pixels color image (thus with an extra $\\times3$ dimensions for the colors channels), associated with a label from one of the classes:\n","\n","```python\n","classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n","```\n","\n","We will divide the dataset in training, testing and validation set. As you already know, the training set will be used to train the model, the validation set will be used to perform model selection and finally, the test set will be used to asses the performance of deep network."],"metadata":{"id":"2Ko0IxcEJdDg"}},{"cell_type":"markdown","source":["**[TO COMPLETE]**\n","\n","In the standard dataset, each pixel intensity is represented by a `uint8` (byte) from $0$ to $255$. As a preprocessing step, we will rescale these values in the range $[0,1]$. You should write a simple so-called MinMaxScaler which takes as input a PIL Image (a specific format for images in Python) and rescales it, after making the appropriate type and shape transformations."],"metadata":{"id":"49Xj_gqoShHl"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"M3RegtIXS8wp"}},{"cell_type":"code","source":["def MinMaxScaler(img):#[TO COMPLETE]\n","  # First, we load the data as numpy array\n","  img_as_array = np.asarray(img)\n","  # TO COMPLETE: Transform the array to Tensor for PyTorch\n","  # TO COMPLETE: image is of size (Height, Width, Channels). But torch excepts (C, H, W). Hence, the second thing is to permute.\n","  # TO COMPLETE: Rescale image pixels form [0,255] to [0,1]\n","  return normalized_img\n","\n","\n","'''\n","NEW DATA LOADING\n","Note that the previous function must be passed to transforms.Lambda().\n","'''\n","dataset = CIFAR10(root='data/', download=True, train=True, transform=transforms.Lambda(MinMaxScaler)) # 50000 samples\n","test_data = CIFAR10(root='data/', download=True, train=False, transform=transforms.Lambda(MinMaxScaler)) # 10000 samples"],"metadata":{"id":"ez87LDF9gkTx","colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["6313c500e458466896364a0fcdb7fd72","01c98be9b3d94d50ad9c1506ad0fa130","167e711dbccf429ca8c519ef4e6c496a","abade8255bb44b2aa2104b9c4861879d","edf73a3d39af4c9c9f2f7692312f87e7","e6723bef5bf1454db80f378ab36692ac","21f1e008c2534388828f97f42f24453b","e6c01a3c087b49ff878e410157579b60","a6ef97e9262e4a26975f8521e343ad6d","491cac5368fc4a1f8c0aeff90891538a","f9343a7a909340e0887ff02d78c037b4"]},"executionInfo":{"status":"ok","timestamp":1681829497707,"user_tz":-330,"elapsed":8765,"user":{"displayName":"Nikhil K C","userId":"09904819297436616315"}},"outputId":"98631da7-2cc0-4c04-e23a-02ef78ab3bfa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6313c500e458466896364a0fcdb7fd72"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data/\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"7GV3m_ZqTNrY"}},{"cell_type":"code","source":["# Let's check the classes and dataset shapes:\n","classes = dataset.classes\n","print(f\"{classes=}\")\n","print(f\"Dataset shape: {dataset.data.shape}\")\n","print(f\"Test set shape: {test_data.data.shape}\")"],"metadata":{"id":"5Xc5SaQoxyfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"dAX4tSnZwf9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's also check whether the dataset is balanced, i.e. there are the same amount of samples for each class\n","label_count = {}\n","for _, idx in dataset:\n","    label = classes[idx]\n","    if label not in label_count:\n","        label_count[label] = 0\n","    label_count[label] += 1\n","label_count"],"metadata":{"id":"nJ_45u1oALt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We split the dataset into training/validation sets:"],"metadata":{"id":"J3SRYMaDehzv"}},{"cell_type":"code","source":["val_size = 5000\n","train_size = len(dataset) - val_size\n","\n","train_data, val_data = random_split(dataset, [train_size, val_size])\n","print(f\"Training samples = {len(train_data)} \\nValidation samples = {len(val_data)} \\nTest samples = {len(test_data)}\")"],"metadata":{"id":"iQPH8tH_iwkG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how normalized sample looks like..."],"metadata":{"id":"TKrBXv2PfFYv"}},{"cell_type":"code","source":["idx = 50 # 51st data sample\n","print(train_data[idx]) # (image, label) tensors\n","assert train_data[idx][0].max().item() <= 1., \"Check your MinMaxScaler!\""],"metadata":{"id":"K3sLGEsRkjYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["... it's better to see it as images!"],"metadata":{"id":"Fm1tImt7fQ1L"}},{"cell_type":"code","source":["img_chan, img_height, img_width = train_data[idx][0].shape #Channel, Height, Width\n","\n","# Plots\n","n_rows = 5\n","n_cols = 10\n","plt.figure(figsize=(n_cols*1.4, n_rows * 1.6))\n","for row in range(n_rows):\n","    for col in range(n_cols):\n","        index = n_cols * row + col\n","        plt.subplot(n_rows, n_cols, index + 1)\n","        image, label = train_data[index]\n","        image = image.permute((1, 2, 0)) #C,H,W -> H,W,C\n","        plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n","        plt.axis('off')\n","        plt.title(classes[label])\n","plt.show()"],"metadata":{"id":"N86f2-vTk859"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Explain in the cell below when using Gradient Descent, why it is usually a good idea to ensure that all the features have a similar scale. "],"metadata":{"id":"HP_A3NzEyHcE"}},{"cell_type":"markdown","source":["`%STARTEXT`"],"metadata":{"id":"VhXlGGmCyR2P"}},{"cell_type":"markdown","source":["Answer: **[TO COMPLETE]**"],"metadata":{"id":"qFLwpkgmySLi"}},{"cell_type":"markdown","source":["`%ENDTEXT`"],"metadata":{"id":"l64Y2StLySWy"}},{"cell_type":"markdown","source":["Since `train_data`, `val_data` and `test_data` are already PyTorch Datasets, we can use directly DataLoaders to load data from them."],"metadata":{"id":"YZwAHcuf2Frd"}},{"cell_type":"code","source":["batch_size = 256\n","dataloader_training = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","dataloader_validation = DataLoader(val_data, batch_size=batch_size)\n","dataloader_test = DataLoader(test_data, batch_size=batch_size)"],"metadata":{"id":"n7xoYIa8GvaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model Definition and Training\n","\n","Let's create a simple CNN. The model will be composed of:\n","* One 2D convolutional layer with kernel size 3x3 and 32 output filters/features, that use ReLU activation function\n","* a Max Pooling layer (2D) of size 2x2.\n","* a Flatten layer\n","* a final Dense layer with 10 output neurons (one per class). We do not need to normalize or transform further the outputs, as the `CrossEntropyLoss` takes care of that. Another equivalent approach would be to add a `LogSoftmax` final layer that returns log-probabilities and then use the `NegativeLogLikelihoodLoss`., and with the _softmax_ activation function to ensure that the sum of all the estimated class probabilities for each image is equal to 1.\n","Note that as `input_shape` attribute's value in the first layer report also the third dimension that represents the channel."],"metadata":{"id":"Lfm2p1PUJnhX"}},{"cell_type":"code","source":["class My_Convolutional_Network(nn.Module):\n","  def __init__(self, conv_filters=[], kernel_sizes=[], max_pool_sizes=[], act_fs=[], verbose=False):\n","    super().__init__()\n","\n","    assert len(conv_filters) == len(kernel_sizes), \"length of {conv_filters} and {kernel_sizes} must be same\"\n","    assert len(conv_filters) == len(max_pool_sizes), \"length of {conv_filters} and {max_pool_sizes} must be same\"\n","    # max pool of [1, 1] corresponds to no max pool\n","    assert len(conv_filters) == len(act_fs), \"length of {conv_filters} and {act_fs} must be same\"\n","\n","    self.conv_layers = nn.ModuleList()\n","    self.max_pools = nn.ModuleList()\n","    self.in_chan = img_chan\n","    self.in_height = img_height\n","    self.in_width = img_width\n","    self.output_dim = len(classes)#10\n","    self.verbose = verbose\n","    self.act_fs = act_fs\n","\n","    height_dimension = self.in_height\n","    width_dimension = self.in_width\n","\n","    for maxp1, maxp2 in max_pool_sizes: # as long as padding='same'\n","      height_dimension = height_dimension // maxp1\n","      width_dimension = width_dimension // maxp2\n","\n","    self.inp_dim_to_linear = conv_filters[-1] * height_dimension * width_dimension\n","\n","    for idx in range(len(conv_filters)):\n","      if idx == 0:\n","        #Conv2d(in_channels, out_channels, kernel_size, padding)\n","        self.conv_layers = self.conv_layers.append(Conv2d(self.in_chan, conv_filters[idx],\n","                                                          kernel_sizes[idx], padding='same'))\n","      else:\n","        self.conv_layers = self.conv_layers.append(Conv2d(conv_filters[idx-1], conv_filters[idx],\n","                                                          kernel_sizes[idx], padding='same'))\n","        \n","      self.max_pools = self.max_pools.append(MaxPool2d(max_pool_sizes[idx]))\n","    \n","    self.linear = Linear(self.inp_dim_to_linear, self.output_dim)\n","\n","  def forward(self, x):\n","\n","    for idx in range(len(self.conv_layers)):\n","      from_shape = x.shape[1:]\n","      active_conv_layer = self.conv_layers[idx]\n","      active_max_pool = self.max_pools[idx]\n","      active_act_fun = self.act_fs[idx]\n","      x = active_max_pool(active_act_fun(active_conv_layer(x)))\n","      to_shape = x.shape[1:]\n","\n","      if self.verbose:\n","        print(f'From dimension [{from_shape}] to dimension [{to_shape}]')\n","\n","    x = torch.flatten(x, start_dim=1) #if start_dim=1 missed, it also consider batch_size\n","\n","    if self.verbose:\n","        print(f'From dimension [{to_shape}] to dimension [{x.shape[1:]}]')\n","        print(f'From dimension [{x.shape[1:]}] to dimension [{self.output_dim}]')\n","\n","    return self.linear(x) #no need to use softmax because of the loss function"],"metadata":{"id":"-X2zBp_BG5iT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_filters = [32]\n","kernel_sizes = [[3, 3]]\n","max_pool_sizes = [[2, 2]]\n","act_fs = [F.relu]\n","\n","num_epochs = 20\n","lr = 1e-3\n","model = My_Convolutional_Network(conv_filters, kernel_sizes, max_pool_sizes,\n","                                 act_fs, False).to(device)\n","summary(model, input_size=(batch_size, 3, 32, 32)) # Here is a nice summary of our model!"],"metadata":{"id":"DsKtE5pOfd84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that the number of parameters mostly depends on the output layers, indeed the parameter sharing techinique used by the 2D convolutional layers allows to significantly reduce the number of learnable weights.\n","Now we can train the model."],"metadata":{"id":"awAy6UY-1jnz"}},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"metadata":{"id":"A4pPhIofg8_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, optimizer, dataloader_train, dataloader_val, epochs):\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(epochs):\n","\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (img, label) in enumerate(dataloader_train):\n","      img, label = img.to(device), label.to(device)\n","      optimizer.zero_grad()\n","      logits = model(img)\n","      loss = criterion(logits, label)\n","      total_loss_train += loss\n","      loss.backward()\n","      optimizer.step()\n","\n","      total_acc_train += (logits.argmax(1) == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","    \n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (img, label) in enumerate(dataloader_val):\n","            img, label = img.to(device), label.to(device)\n","            logits = model(img)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            total_acc_val += (logits.argmax(1) == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val) \n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","  return loss_train, acc_train, loss_val, acc_val"],"metadata":{"id":"-GnNWzZrjVDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, epochs = num_epochs)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"],"metadata":{"id":"Uy-bG-y2qJRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_learning_acc_and_loss(loss_tr, acc_tr, loss_val, acc_val):\n","\n","    plt.figure(figsize=(8, 10))\n","\n","    plt.subplot(2, 1, 1)\n","    plt.grid()\n","    plt.plot(range(num_epochs), acc_tr, label='acc_training')\n","    plt.plot(range(num_epochs), acc_val, label='acc_validation')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='best')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.grid()\n","    plt.plot(range(num_epochs), loss_tr, label='loss_training')\n","    plt.plot(range(num_epochs), loss_val, label='loss_validation')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend(loc='best')\n","\n","    plt.show()"],"metadata":{"id":"d-pu3mDwqLya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"],"metadata":{"id":"d00oUM1q951h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we test the model:"],"metadata":{"id":"nnu_aFjRAZRv"}},{"cell_type":"code","source":["def test(model, dataloader_test=dataloader_test):\n","  model.eval()\n","  total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n","  for idx, (img, label) in enumerate(dataloader_test):\n","      img, label = img.to(device), label.to(device)\n","      logits = model(img)\n","      loss += criterion(logits, label)\n","      total_acc_test += (logits.argmax(1) == label).sum().item()\n","      total_count_test += label.size(0)\n","      n_batches_test += 1\n","  accuracy_test = total_acc_test/total_count_test\n","  loss_test = loss/n_batches_test\n","  print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n","  print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")"],"metadata":{"id":"5x7_x8X1nF3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(model)"],"metadata":{"id":"sQ4rNwrMq-6r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we see that the test accuracy of our simple CNN is about 64%."],"metadata":{"id":"F3a4YlGm1RQ8"}},{"cell_type":"markdown","source":["## Visualize filters\n","Let's now visualize the filters emerged in the first convolutional layer."],"metadata":{"id":"u9431xgoThpf"}},{"cell_type":"code","source":["layer_names = []\n","weights = {}\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        layer_names.append(name)\n","        weights.setdefault(name, param.data)\n","\n","conv_weights = weights['conv_layers.0.weight']\n","print(f\"{conv_weights.shape=} <=> [out_dim, inp_dim, kernel_size[0], kernel_size[1]]\")"],"metadata":{"id":"lNLIXlsVPcPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_w0 = conv_weights[:, 0, :, :] # Pick one input dimension\n","\n","# Rescale weights for visualization\n","conv_w0 -= torch.min(conv_w0)\n","conv_w0 /= torch.max(conv_w0)\n","\n","for r in range(4):\n","    for c in range(8):\n","        n=r*8+c\n","        plt.subplot(4, 8, n+1)\n","        plt.imshow(conv_w0[n,:,:].cpu(), interpolation='none')\n","        plt.title(f'chan{n+1}')\n","        plt.axis('off')\n","        plt.gray()\n","plt.show()"],"metadata":{"id":"AKtAqvuwT3Q0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["They might be a bit hard to interpret, but it seems that the various filters have learned to detect various corners and edges."],"metadata":{"id":"HgWPQbYbuGA9"}},{"cell_type":"markdown","source":["## Parameters of a CNN [TO COMPLETE]"],"metadata":{"id":"SwY5uAFZTlEX"}},{"cell_type":"markdown","source":["Explain in the cell below:\n","\n","1. How is the number of parameters on each of the two Conv2D layers determined?\n","\n","2. Why do the two considered convolutional layers have a different number of parameters?"],"metadata":{"id":"5tiNlSeZtezn"}},{"cell_type":"markdown","source":["`%STARTEXT`"],"metadata":{"id":"D3iMaFMwswsY"}},{"cell_type":"markdown","source":["Answer1 : **[TO COMPLETE]**\n","\n","Answer2 : **[TO COMPLETE]**"],"metadata":{"id":"qIMuYZX2suKM"}},{"cell_type":"markdown","source":["`%ENDTEXT`"],"metadata":{"id":"GimDOIlNszFW"}},{"cell_type":"markdown","source":["# Exercise 3.2: Deep CNN\n","\n","\n","Let's consider a deeper model, more precisly in this exercise we consider a model composed of:\n","* One 2D convolutional layer with kernel size 3x3 and 32 output filters/features, that use ReLu activation function\n","* a Max Pooling layer (2D) of size 2x2 \n","* One 2D convolutional layer with kernel size 2x2 and 16 output filters/features, that use ReLu activation function\n","* a Max Pooling layer (2D) of size 2x2\n","* a Flatten layer\n","* a final Dense layer with 10 output neurons (one per class), and with the _softmax_ activation function\n"],"metadata":{"id":"dZtcrs38XM3m"}},{"cell_type":"code","source":["conv_filters = [32, 16]\n","kernel_sizes = [[3, 3], [2, 2]]\n","max_pool_sizes = [[2, 2], [2, 2]]\n","act_fs = [F.relu, F.relu]\n","\n","num_epochs = 20\n","lr = 1e-3\n","deep_model = My_Convolutional_Network(conv_filters, kernel_sizes, max_pool_sizes,\n","                                      act_fs, False).to(device)\n","summary(deep_model, input_size=(batch_size, 3, 32, 32)) # Here is a nice summary of our model!"],"metadata":{"id":"Ye2w8dCuXSQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(deep_model.parameters(), lr=lr)"],"metadata":{"id":"NKSNZijCVJSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(deep_model, optimizer, dataloader_training, dataloader_validation, epochs = num_epochs)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"],"metadata":{"id":"iOPNAMWHYuw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"],"metadata":{"id":"NT6OiKCwY0R5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(deep_model)"],"metadata":{"id":"BbAcxuQkoBT_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Whoa! By developing the deep CNN we dropped the accuracy fom ~64% to ~62%. It seems like we need to develop a better CNN model. "],"metadata":{"id":"jskjJB_93Xm6"}},{"cell_type":"markdown","source":["# Exercise 3.3: A better CNN [TO COMPLETE]\n","\n","Let's develop a network that performs better than the very simple one above. This exercise aims to explore how much the various hyper-parameters influence the classification capability of the model. \n","\n","**[TO COMPLETE]**: Your task is to modify some of the hyper-parameters of the previous exercise's network and compare the results. At least one of the models you try should have an improvement in the test set results (generalization) over the result of the model used in the previous exercise.\n","In the cell below report only the code of the **best model** that you can find. In addtion, print out its result on the test set, and plot the accuracy and the loss trends in the notebook you return.\n","Moreover, for each setup you test, analyze and discuss the obtained results briefly in the last cells at the bottom.\n","\n","Hint: Each reparameterization should change a different aspect in the network, while the rest of the parameters would stay the same. \n","Example parameters to try to change (we suggest to test at least one re-parametrization for each of these categories):\n","\n","*    number of layers or neurons or filters dimension\n","*   activation functions\n","*   epochs\n","*   batch sizes\n","*   max-pooling on/off on certain layers, or pool size"],"metadata":{"id":"tYiMa1cxaB2m"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"Qa6mcdofREf5"}},{"cell_type":"code","source":["# TO COMPLETE"],"metadata":{"id":"YKk437rNRDsO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"tUijjhBRREaf"}},{"cell_type":"markdown","source":["# [TO COMPLETE] Discuss your results\n","\n","In the discussion, you need to clearly motivate your choice of hyperparameters, what work and what did not work. \n","You also need to plot all the results of experiments you have conducted. "],"metadata":{"id":"RQcbxs6NRU9Q"}},{"cell_type":"markdown","source":["`%STARTEXT`"],"metadata":{"id":"Of_nb4jqRXCi"}},{"cell_type":"markdown","source":["The best model that I found is... **[TO COMPLETE]**\n","\n","The achieved accuracy in the test set is... **[TO COMPLETE]**\n","\n","Discussion:\n","**[TO COMPLETE]**"],"metadata":{"id":"RzlXX86y6VaK"}},{"cell_type":"markdown","source":["Besides, I tested also other models: \n","* **[TO COMPLETE]**\n","* ..\n","\n","\n","Discussion:\n","**[TO COMPLETE]**"],"metadata":{"id":"j_aCzHZh6bCe"}},{"cell_type":"markdown","source":["`%ENDTEXT`"],"metadata":{"id":"si3hFOSIRZvr"}},{"cell_type":"markdown","source":["# Transfer Learning with ResNet18 + ImageNet"],"metadata":{"id":"7nl9nbhsxgyn"}},{"cell_type":"markdown","source":["Let's now experiment with transfer learning. We will now load the model structure and weights of a small [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) (which is still pretty big!), pretrained on the ImageNet dataset. We will then add a fully connected layer at the end of the network and fine-tune it on the CIFAR10 dataset. In this way, we can leverage the knowledge already present in the pre-trained weight and transfer it on our task!"],"metadata":{"id":"ixsJSGmwGjGA"}},{"cell_type":"code","source":["from torchvision.models import resnet18, ResNet18_Weights"],"metadata":{"id":"rYi1ipiUFqMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["RESNET18 = resnet18(pretrained=True)\n","summary(RESNET18, input_size=(batch_size, 3, 32, 32))"],"metadata":{"id":"dj4x39INycYi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The ResNet has a final fully connected layer that generates 1000 logits corresponding to the classes in the ImageNet dataset. We will re-define it and make it map the features learned in the previous layer to the 10 classes of the CIFAR10 datset. "],"metadata":{"id":"XpWkgXIeHnsj"}},{"cell_type":"code","source":["linear_layer_input = RESNET18.fc.in_features\n","linear_layer_output = RESNET18.fc.out_features\n","print(f\"Last layer: {linear_layer_input} -> {linear_layer_output}\")"],"metadata":{"id":"xWpT1c3G2vU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace  the last layer\n","RESNET18.fc = Linear(linear_layer_input, 10)"],"metadata":{"id":"bZMjSbVL34JY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["linear_layer_input = RESNET18.fc.in_features\n","linear_layer_output = RESNET18.fc.out_features\n","print(f\"Last layer: {linear_layer_input} -> {linear_layer_output}\")"],"metadata":{"id":"yDswGss14CMF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we \"freeze\" the other weights in the network so that only the ones in the last layer are trainable."],"metadata":{"id":"ZU8itp1eH8WC"}},{"cell_type":"code","source":["for name, param in RESNET18.named_parameters():\n","    if name == 'fc.weight' or name == 'fc.bias':\n","      param.requires_grad = True\n","    else:\n","      param.requires_grad = False"],"metadata":{"id":"7anuWdD62b5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["RESNET18 = RESNET18.to(device) # Let's now load it to GPU"],"metadata":{"id":"3Yzd6yG_yREw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's consider a smaller subset of CIFAR10 in order to speed-up the fine-tuning process:"],"metadata":{"id":"-eksr6l6LOP8"}},{"cell_type":"code","source":["train_idxs = list(range(0, len(train_data), 5))  # 45000/5=9000 Tr\n","val_idxs = list(range(1, len(val_data), 5))  # 5000/5=1000 Val\n","test_idxs = list(range(1, len(test_data), 5))  # 10000/5=2000 Ts\n","\n","train_data_sub = torch.utils.data.Subset(train_data, train_idxs)\n","val_data_sub = torch.utils.data.Subset(val_data, val_idxs)\n","test_data_sub = torch.utils.data.Subset(test_data, test_idxs)"],"metadata":{"id":"Yfv8a8j48-zm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to apply custom preprocessing steps that are expected by the pretrained model, we will define a small wrapper around the CIFAR10 dataset."],"metadata":{"id":"7H5JC628Ndz5"}},{"cell_type":"code","source":["weights = ResNet18_Weights.DEFAULT\n","preprocess = weights.transforms()\n","\n","class RESDatasetWrapper(Dataset):\n","  def __init__(self, dataset):\n","    self.dataset = dataset\n","  def __len__(self):\n","    return len(self.dataset)\n","  def __getitem__(self, idx):\n","    image, label = self.dataset[idx]\n","    image = preprocess(image)\n","    label = torch.tensor(label, dtype=torch.long)\n","    return image.to(device), label.to(device)"],"metadata":{"id":"G-X74d0-Gf4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["RES_train_dataset = RESDatasetWrapper(train_data_sub)\n","RES_val_dataset = RESDatasetWrapper(val_data_sub)\n","RES_test_dataset = RESDatasetWrapper(test_data_sub)\n","\n","batch_size=256\n","\n","RES_dataloader_training = DataLoader(RES_train_dataset, batch_size=batch_size, shuffle=True)\n","RES_dataloader_validation = DataLoader(RES_val_dataset, batch_size=batch_size)\n","RES_dataloader_test = DataLoader(RES_test_dataset, batch_size=batch_size)"],"metadata":{"id":"2ktgFiaQGd8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","lr = 1e-3\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(RESNET18.parameters(), lr=lr)"],"metadata":{"id":"DmY9GHEKxyd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(RESNET18, optimizer,\n","                                                           RES_dataloader_training, \n","                                                           RES_dataloader_validation,\n","                                                           epochs = num_epochs)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"],"metadata":{"id":"Sj8vU7Yf8RNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"],"metadata":{"id":"wQKYIbnOAumj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(RESNET18, dataloader_test=RES_dataloader_test)"],"metadata":{"id":"MhzLq8jUAwcA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice how we got this result by leveraging a model freely available on the internet and trained on another dataset. That's the power of transfer learning! "],"metadata":{"id":"rMucQVvEH4ZZ"}}]}